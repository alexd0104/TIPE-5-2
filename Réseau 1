# -*- coding: utf-8 -*-
"""
Created on Wed May 22 16:44:54 2024

@author: alexf
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Mar 27 10:46:26 2024

@author: alexf
"""

bareme=1

map1="""1hhhhhhhhthhhhhhhh2
v99999999v99999999v
v9<>9<h>9u9<h>9<>9v
v99999999999999999v
v9<>9n9<hth>9n9<>9v
v9999v999v999v9999v
3hh29xh>9u9<hy91hh4
888v9v9999999v9v888
hhh49u91>9<29u93hhh
9999999v999v9999999
hhh29n93hhh49n91hhh
888v9v9999999v9v888
1hh49u9<hth>9u93hh2
v99999999v99999999v
v9<29<h>9u9<h>91>9v
v99v99999999999v99v
x>9u9n9<hth>9n9u9<y
v9999v999v999v9999v
v9<hhjh>9u9<hjhh>9v
v99999999999999999v
3hhhhhhhhhhhhhhhhh4"""

import tensorflow as tf
from tensorflow.keras import models, layers
import numpy as np
import time
import matplotlib.pyplot as plot
import fonction_adaptee3 as fa
import recompenses as rw
import ville_to_graphe as vg


tab_s=[]
nbr_action=tf.constant(4) 
file_model='modele_1.keras'
file_stats='graphe_score_1'

gamma=tf.constant(0.999)
epoch=1500
#decalage_debut=90
taille_sequence=2
nbr_jeu=40
best_score=0

epsilon=1.
epsilon_min=0.10
start_epsilon=1
end_epsilon=epoch//4
epsilon_decay_value=epsilon/(end_epsilon-start_epsilon)

def model(nbr_cc=8):
  entree=layers.Input(shape=( 650, 570, taille_sequence,), dtype='float32')
  #pour 1 convolution : le noyau de convolution est de taille 3X3 et se déplace de 2 en 2.
  result=layers.Conv2D(  nbr_cc, 3, activation='relu', padding='same', strides=2)((entree/128)-1) #on normalise les pixels dans [-1,1]
  result=layers.Conv2D(2*nbr_cc, 3, activation='relu', padding='same', strides=2)(result)
  result=layers.BatchNormalization()(result)
  result=layers.Conv2D(4*nbr_cc, 3, activation='relu', padding='same', strides=2)(result)
  result=layers.Conv2D(8*nbr_cc, 3, activation='relu', padding='same', strides=2)(result)
  result=layers.BatchNormalization()(result)

  result=layers.Flatten()(result)

  result=layers.Dense(512, activation='relu')(result)
  sortie=layers.Dense(4)(result)
    
  model=models.Model(inputs=entree, outputs=sortie)
  return model

def red_scale(image):
   image=np.array(image)
   red=image[:,:,0]
   res=np.expand_dims(red,axis=-1)
   return res

def simulation(epsilon, debug=False):
  if debug:
    start_time=time.time()

  tab_observations=[]
  tab_rewards=[]
  tab_actions=[]
  tab_next_observations=[]
  tab_done=[]
  done=False
  vie=3
  tour=0
  observation, Coord, Gomme, tour, points, peur, peurs, killstreak, catch, new_vie =fa.game_reset()
  img1,img2=red_scale(observation[0]),red_scale(observation[1])
  tab_sequence=[img1,img2]
  tab_sequence=np.array(tab_sequence, dtype=np.float32)
  ######

  score=0
  while True:
    if np.random.random()>epsilon:
      sequence_entree= tab_sequence.transpose(0,4,1,2,3)
      valeurs_q=model(sequence_entree)
      action=int(tf.argmax(valeurs_q[0]))
    else:
      action=np.random.randint(0, nbr_action)

    h=np.random.randint(1) #on garde que 10% des données pour les décoréler
    if h==0:
      tab_sequence=np.array(tab_sequence)
      tab_observations.append(np.concatenate(tab_sequence, axis=-1))
      tab_actions.append(action)
    score+=points
    
    if fa.win(Gomme):
        done=True
        
    malus=0
    if new_vie<vie:
      malus= rw.reward_vie(bareme,new_vie)
      vie=new_vie
      if h==0:
          tab_done.append(True) #done est toujours faux mais dans le calcul de Bellman on aura True (donc 1)
    elif h==0:
        tab_done.append(done)
    if vie==0:
        done=True
    
    
    reward = points + malus
    if h==0:
      if reward==0:
          reward=-1
      tab_rewards.append(reward)
    
    if done:
      tab_s.append(score)
      if h==0:
        tab_sequence=[img1,img2]
        tab_next_observations.append(np.concatenate(tab_sequence, axis=-1))
      
      tab_done=np.array(tab_done, dtype=np.float32)
      tab_observations=np.array(tab_observations, dtype=np.float32)
      tab_next_observations=np.array(tab_next_observations, dtype=np.float32)
      tab_rewards=np.array(tab_rewards, dtype=np.float32)
      tab_actions=np.array(tab_actions, dtype=np.int32)
      if debug:
            print("  Creation observations {:5.3f} seconde(s)".format(float(time.time()-start_time)))
            print("     score:{:5d}   batch:{:4d}".format(int(score), len(tab_done)))
      return tab_observations,\
             tab_rewards,\
             tab_actions,\
             tab_next_observations,\
             tab_done
             
    va_jeu= observation, Coord, Gomme, tour, points, peur, peurs, killstreak, catch, vie
    observation, Coord, Gomme, tour, points, peur, peurs, killstreak, catch, vie = fa.take_action(action+1, va_jeu)
    img1,img2=red_scale(observation[0]), red_scale(observation[1]) 
    tab_sequence=[img1,img2]
    if h==0:
      tab_next_observations.append(np.concatenate(tab_sequence, axis=-1))

  
  
  
  
  
  
  
  
def my_loss(y, q):
  loss=tf.reduce_mean(tf.math.square(y-q))
  return loss

@tf.function
def train_step(reward, action, observation, next_observation, done):
  next_Q_values=model(next_observation)
  best_next_actions=tf.math.argmax(next_Q_values, axis=1)
  next_mask=tf.one_hot(best_next_actions, nbr_action)
  next_best_Q_values=tf.reduce_sum(next_Q_values*next_mask, axis=1)
  target_Q_values=reward+(1-done)*gamma*next_best_Q_values
  target_Q_values=tf.reshape(target_Q_values, (-1, 1))
  mask=tf.one_hot(action, nbr_action)
  with tf.GradientTape() as tape:
    all_Q_values=model(observation)
    Q_values=tf.reduce_sum(all_Q_values*mask, axis=1, keepdims=True)
    loss=my_loss(target_Q_values, Q_values)
  gradients=tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
  train_loss(loss)
  
def train(debug=False):
  global epsilon, best_score
  for e in range(epoch):
    for i in range(nbr_jeu):
      print("Epoch {:04d}/{:05d} epsilon={:05.3f}".format(i, e, epsilon))
      tab_observations, tab_rewards, tab_actions, tab_next_observations, tab_done=simulation(epsilon, debug=True)
      if debug:
        start_time=time.time()
      train_step(tab_rewards, tab_actions, tab_observations, tab_next_observations, tab_done)
      if debug:
        print("  Entrainement {:5.3f} seconde(s)".format(float(time.time()-start_time)))
        print("     loss: {:6.4f}".format(train_loss.result()))
      train_loss.reset_state()

    epsilon-=epsilon_decay_value
    epsilon=max(epsilon, epsilon_min)
    np.save(file_stats, tab_s)
    if np.mean(tab_s[-200:])>best_score: #(on regarde les 200 derniers scores -> optimisable ?
      print("Sauvegarde du modele")
      model.save(file_model)
      best_score=np.mean(tab_s[-200:])

model=model(16)

optimizer=tf.keras.optimizers.Adam(learning_rate=1E-4)
train_loss=tf.keras.metrics.Mean()
tab_s=[]
train(debug=True)
  
"""
observation, Coord, Gomme, tour, points, peur, peurs, killstreak, catch, new_vie =fa.game_reset()
#for i in range(decalage_debut-taille_sequence):
#  print('là')
#  take_action('0',va_jeu)
tab_sequence=[]
img1,img2=red_scale(observation[0]),red_scale(observation[1])
tab_sequence.append([img1,img2])
tab_sequence.append([img1,img2])
tab_sequence=np.array(tab_sequence, dtype=np.float32)
new_array = tab_sequence.transpose(0,4,1,2,3)
valeurs_q=model(new_array[0])
action=int(tf.argmax(valeurs_q[0]))

"""
